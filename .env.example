# ==============================================================================
# DEEP AGENT ONE - ENVIRONMENT CONFIGURATION
# ==============================================================================
#
# This file contains all configuration settings for the Deep Agent One project.
# Copy this file to `.env` and replace placeholder values with your actual credentials.
#
# IMPORTANT: Never commit your `.env` file - it should remain local and git-ignored.
#
# ==============================================================================

# ==============================================================================
# ENVIRONMENT & DEBUG SETTINGS
# ==============================================================================
#
# ENV: Deployment environment identifier
#   - local: Development on your local machine (default, safe)
#   - dev: Shared development server
#   - staging: Pre-production testing environment
#   - prod: Production deployment
#   - test: Automated testing (CI/CD)
#
# DEFAULT: local (safe for development)
# PRODUCTION: Set to 'prod' only in production deployments
ENV=local

# DEBUG: Enable debug logging and verbose output
#   true: Detailed logs, stack traces, debug toolbar (development)
#   false: Production-grade logging only (staging/prod)
#
# DEFAULT: false (safe default)
# RECOMMENDED: true for local/dev, false for staging/prod
DEBUG=false

# ==============================================================================
# PRIMARY MODEL: GEMINI 3 PRO (GOOGLE)
# ==============================================================================
#
# Gemini 3 Pro is the primary model for all agent operations.
# Get your API key at: https://aistudio.google.com/apikey
#
# SECURITY: This key provides access to your Google AI account and incurs costs.
# Protect it like a password. Never commit it to version control.
GOOGLE_API_KEY=your_google_api_key_here

# GEMINI_MODEL_NAME: The Google Gemini model to use
# DEFAULT: gemini-3-pro-preview
GEMINI_MODEL_NAME=gemini-3-pro-preview

# GEMINI_TEMPERATURE: Temperature for sampling
# DEFAULT: 1.0
# WARNING: Keep at 1.0 per Google docs - lower values can cause looping or
# degraded performance on complex reasoning tasks.
GEMINI_TEMPERATURE=1.0

# GEMINI_THINKING_LEVEL: Thinking depth for Gemini 3 Pro
#   - low: Quick responses
#   - medium: Balanced reasoning
#   - high: Deep reasoning (default)
#
# DEFAULT: high
GEMINI_THINKING_LEVEL=high

# GEMINI_MAX_OUTPUT_TOKENS: Maximum tokens in model response
# DEFAULT: 4096
GEMINI_MAX_OUTPUT_TOKENS=4096

# ==============================================================================
# FALLBACK MODEL: GPT-5.1 (OPENAI)
# ==============================================================================
#
# GPT-5.1 serves as the fallback model when Gemini fails.
# Get your API key at: https://platform.openai.com/api-keys
#
# SECURITY: This key provides access to your OpenAI account and incurs costs.
# Protect it like a password. Never commit it to version control.
OPENAI_API_KEY=your_openai_key_here

# GPT_MODEL_NAME: The OpenAI model to use as fallback
# DEFAULT: gpt-5.1-2025-11-13 (GPT-5.1 dated release)
GPT_MODEL_NAME=gpt-5.1-2025-11-13

# GPT_DEFAULT_REASONING_EFFORT: Default reasoning depth for GPT models
#   - minimal: Quick responses, lower cost (simple queries)
#   - low: Basic reasoning (routine tasks)
#   - medium: Balanced reasoning (most queries)
#   - high: Deep reasoning (complex problems)
#
# DEFAULT: medium
GPT_DEFAULT_REASONING_EFFORT=medium

# GPT_DEFAULT_VERBOSITY: Response detail level
#   - low: Concise, direct answers
#   - medium: Balanced explanations
#   - high: Detailed, comprehensive responses
#
# DEFAULT: medium
GPT_DEFAULT_VERBOSITY=medium

# GPT_MAX_TOKENS: Maximum tokens in model response
# DEFAULT: 4096
GPT_MAX_TOKENS=4096

# ==============================================================================
# MODEL FALLBACK CONFIGURATION
# ==============================================================================
#
# Automatic fallback from Gemini to GPT on errors (rate limits, timeouts, etc.)
#
# ENABLE_MODEL_FALLBACK: Enable automatic model fallback
# DEFAULT: true
# BENEFIT: Resilience - agent continues working even if primary model fails
ENABLE_MODEL_FALLBACK=true

# ==============================================================================
# REASONING SYSTEM CONFIGURATION
# ==============================================================================
#
# Dynamic reasoning system adjusts GPT-5 effort based on query complexity
# Automatically detects trigger phrases and escalates reasoning depth
#
# ENABLE_DYNAMIC_REASONING: Enable automatic reasoning effort adjustment
# DEFAULT: true
# RECOMMENDED: true (cost optimization)
ENABLE_DYNAMIC_REASONING=true

# REASONING TIMEOUTS: Maximum execution time per reasoning level (seconds)
#   - MINIMAL: Quick queries, simple lookups (5s)
#   - LOW: Basic tasks, routine operations (15s)
#   - MEDIUM: Standard complexity (30s)
#   - HIGH: Deep analysis, complex reasoning (60s)
#
# NOTE: These timeouts prevent runaway costs and ensure responsiveness
REASONING_MINIMAL_TIMEOUT=5
REASONING_LOW_TIMEOUT=15
REASONING_MEDIUM_TIMEOUT=30
REASONING_HIGH_TIMEOUT=60

# TRIGGER_PHRASES: Comma-separated phrases that trigger high reasoning effort
# DEFAULT: "think harder,deep dive,analyze carefully,be thorough"
# CUSTOMIZE: Add domain-specific phrases for your use case
TRIGGER_PHRASES="think harder,deep dive,analyze carefully,be thorough"

# COMPLEXITY_THRESHOLD_HIGH: Score threshold for high reasoning (0.0-1.0)
# DEFAULT: 0.8 (only highly complex queries)
COMPLEXITY_THRESHOLD_HIGH=0.8

# COMPLEXITY_THRESHOLD_MEDIUM: Score threshold for medium reasoning (0.0-1.0)
# DEFAULT: 0.5 (balanced threshold)
COMPLEXITY_THRESHOLD_MEDIUM=0.5

# ==============================================================================
# TOKEN CONSERVATION SETTINGS
# ==============================================================================
#
# Optimize token usage to reduce costs while maintaining quality
#
# ENABLE_RESPONSE_CHAINING: Break long responses into multiple API calls
# DEFAULT: true
# BENEFIT: Reduces per-request token count, improves streaming
ENABLE_RESPONSE_CHAINING=true

# MAX_CHAIN_LENGTH: Maximum number of chained responses
# DEFAULT: 3
# RANGE: 1-10 (higher = more granular but more API calls)
MAX_CHAIN_LENGTH=3

# ENABLE_CONTEXT_COMPRESSION: Compress conversation history before sending
# DEFAULT: true
# BENEFIT: Reduces token count for long conversations
ENABLE_CONTEXT_COMPRESSION=true

# ==============================================================================
# EXTERNAL AI SERVICES
# ==============================================================================

# PERPLEXITY: Web search and research capabilities
# Get your API key at: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=your_perplexity_key_here

# ==============================================================================
# MONITORING & TRACING (LANGSMITH)
# ==============================================================================
#
# LangSmith provides observability for agent operations
# Sign up at: https://smith.langchain.com/
#
# LANGSMITH_API_KEY: Your LangSmith API key (optional but recommended)
LANGSMITH_API_KEY=your_langsmith_key_here

# LANGSMITH_PROJECT: Project name in LangSmith dashboard
# DEFAULT: deep-agent-one
LANGSMITH_PROJECT=deep-agent-one

# LANGSMITH_ENDPOINT: LangSmith API endpoint
# DEFAULT: https://api.smith.langchain.com (no need to change)
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# LANGSMITH_TRACING_V2: Use LangSmith tracing v2 protocol
# DEFAULT: true (recommended)
LANGSMITH_TRACING_V2=true

# ==============================================================================
# PROMPT OPTIMIZATION (OPIK)
# ==============================================================================
#
# Opik provides automated prompt optimization and A/B testing
# Get your workspace at: https://www.comet.com/site/products/opik/
#
# OPIK_API_KEY: Your Opik/Comet API key (optional, Phase 1)
OPIK_API_KEY=your_opik_key_here

# OPIK_WORKSPACE: Your Opik workspace name
OPIK_WORKSPACE=your_workspace_here

# OPIK_PROJECT: Project name for prompt experiments
# DEFAULT: deep-agent-reasoning
OPIK_PROJECT=deep-agent-reasoning

# ==============================================================================
# DATABASE CONFIGURATION (PHASE 1)
# ==============================================================================
#
# PostgreSQL with pgvector for vector storage and semantic search
# Currently unused in Phase 0 - implemented in Phase 1
#
# DATABASE_URL: PostgreSQL connection string
# FORMAT: postgresql://user:password@host:port/dbname
DATABASE_URL=postgresql://user:password@host:port/dbname

POSTGRES_USER=your_db_user
POSTGRES_PASSWORD=your_db_password
POSTGRES_DB=deep_agent_one

# ENABLE_PGVECTOR: Enable pgvector extension for vector storage
# DEFAULT: true (Phase 1)
ENABLE_PGVECTOR=true

# ==============================================================================
# CHECKPOINTER CONFIGURATION (LANGGRAPH STATE PERSISTENCE)
# ==============================================================================
#
# LangGraph checkpointer stores agent state for recovery and debugging
#
# CHECKPOINT_DB_PATH: SQLite database path for checkpoints
# DEFAULT: data/checkpoints.db
CHECKPOINT_DB_PATH=data/checkpoints.db

# CHECKPOINT_CLEANUP_DAYS: Auto-delete checkpoints older than N days
# DEFAULT: 7 days
# RECOMMENDED: 7 for local, 30 for staging/prod (regulatory compliance)
CHECKPOINT_CLEANUP_DAYS=7

# ==============================================================================
# STREAMING CONFIGURATION (AGENT EVENT STREAMING)
# ==============================================================================
#
# Agent streaming provides real-time event updates to the frontend via WebSocket
#
# STREAM_VERSION: LangGraph event format version
#   - v1: Legacy format (deprecated)
#   - v2: Current format with shard separation (recommended)
#
# DEFAULT: v2
STREAM_VERSION=v2

# STREAM_ALLOWED_EVENTS: Comma-separated list of events to stream to frontend
# NOTE: These events are filtered from LangGraph's event stream
#
# CRITICAL EVENTS:
#   - on_chat_model_stream: Streaming text tokens from GPT-5
#   - on_chat_model_end: Marks shard boundaries (UI separation)
#   - on_tool_start/end: Tool execution lifecycle
#   - on_tool_call_start/end: LangGraph v2 tool events
#
# NOTE: WebSocket endpoint (/api/v1/ws) is excluded from HTTP timeout middleware (30s)
# to allow longer streaming operations. Agent streaming has its own timeout below.
STREAM_ALLOWED_EVENTS=on_chat_model_stream,on_chat_model_end,on_tool_start,on_tool_end,on_tool_call_start,on_tool_call_end,on_chain_start,on_chain_end,on_llm_start,on_llm_end

# STREAM_TIMEOUT_SECONDS: Maximum time for agent execution before timeout
#   Increased timeout to support parallel tool execution + GPT-5 synthesis
#
# DEFAULT: 300 seconds (5 minutes)
# REASONING: 300s provides safety margin for:
#   - GPT-5 API rate limits and retries
#   - Parallel tool execution (up to 6 tools)
#   - Cold start latency for external services
#   - Network variability
#
# RECOMMENDED VALUES:
#   - local: 300 (generous for debugging)
#   - test: 60 (CI/CD speed)
#   - staging/prod: 180 (production SLA)
STREAM_TIMEOUT_SECONDS=300

# TOOL_EXECUTION_TIMEOUT: Per-tool execution timeout (seconds)
# DEFAULT: 45 seconds
# NOTE: Must be < STREAM_TIMEOUT_SECONDS to prevent race conditions
# NOTE: Currently not enforced by LangGraph (advisory only)
TOOL_EXECUTION_TIMEOUT=45

# WEB_SEARCH_TIMEOUT: Timeout for web search tool (Perplexity MCP)
# DEFAULT: 30 seconds
# REASONING: Perplexity typically responds within 5-15s
WEB_SEARCH_TIMEOUT=30

# MAX_TOOL_CALLS_PER_INVOCATION: Maximum tool calls per agent invocation
# DEFAULT: 6
# REASONING: Prevents infinite loops and runaway costs
#   - Each tool call = 2 LangGraph recursion steps (LLM call + tool execution)
#   - 6 tool calls = ~12 recursion steps (well below 25-step limit)
#   - Agent gracefully completes reasoning after Nth tool call
MAX_TOOL_CALLS_PER_INVOCATION=6

# ==============================================================================
# WEBSOCKET & EVENT PROCESSING TIMEOUTS
# ==============================================================================
#
# Fine-grained timeouts for WebSocket connection management
#
# HEARTBEAT_INTERVAL_SECONDS: Frequency of keep-alive messages
#   Lower = faster disconnect detection but more network overhead
#
# DEFAULT: 5 seconds
# RECOMMENDED: 3-10s depending on network reliability
HEARTBEAT_INTERVAL_SECONDS=5

# EVENT_QUEUE_TIMEOUT_SECONDS: Max wait for new events during multiplexing
#   Lower = higher CPU usage but faster event delivery
#
# DEFAULT: 1.0 seconds
# RECOMMENDED: 0.5-2.0s depending on responsiveness requirements
EVENT_QUEUE_TIMEOUT_SECONDS=1.0

# CHECKPOINT_GRACE_PERIOD_SECONDS: Time allowed for state persistence during shutdown
#   Should exceed typical database write latency
#
# DEFAULT: 5.0 seconds
# RECOMMENDED: 3-10s depending on database performance
CHECKPOINT_GRACE_PERIOD_SECONDS=5.0

# ==============================================================================
# REDIS CACHE (PHASE 2)
# ==============================================================================
#
# Redis for caching and rate limiting
# Currently unused in Phase 0 - implemented in Phase 2
#
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=your_redis_password

# CACHE_TTL: Cache time-to-live in seconds
# DEFAULT: 3600 (1 hour)
CACHE_TTL=3600

# ==============================================================================
# FASTAPI BACKEND CONFIGURATION
# ==============================================================================
#
# FastAPI server settings for the Deep Agent One backend
#
# API_HOST: Network interface to bind to
#   - 0.0.0.0: All interfaces (container-friendly, production)
#   - 127.0.0.1: Localhost only (development, secure)
#
# DEFAULT: 0.0.0.0
# SECURITY: Ensure firewall rules are in place for production deployments
API_HOST=0.0.0.0

# API_PORT: Port to listen on
# DEFAULT: 8000
API_PORT=8000

# API_RELOAD: Enable auto-reload on code changes (development only)
#   true: Hot reload (development)
#   false: No reload (production)
#
# DEFAULT: false
# RECOMMENDED: true for local/dev, false for staging/prod
API_RELOAD=false

# CORS_ORIGINS: Comma-separated list of allowed origins for CORS
# DEFAULT: http://localhost:3000,http://localhost:8000
# PRODUCTION: Update with your actual frontend domain
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# API_VERSION: API version prefix for endpoints
# DEFAULT: v1
API_VERSION=v1

# ==============================================================================
# FRONTEND CONFIGURATION (NEXT.JS - BROWSER-EXPOSED)
# ==============================================================================
#
# IMPORTANT: These NEXT_PUBLIC_* variables are embedded in the browser bundle.
# They are accessible in React components via process.env.NEXT_PUBLIC_*
#
# WARNING: Do NOT put secrets here - they will be visible to all users!
#
# These settings are read by the Next.js frontend at BUILD TIME and exposed
# to the browser. They configure how the frontend connects to the backend.
#
# NOTE: Frontend reads these from the ROOT .env file (this file).
# You do NOT need to create a separate frontend/.env.local file.

# NEXT_PUBLIC_API_URL: Backend API base URL
# DEFAULT: http://localhost:8000
# PRODUCTION: Update with your actual backend domain
NEXT_PUBLIC_API_URL=http://localhost:8000

# NEXT_PUBLIC_WS_URL: WebSocket URL for streaming
# DEFAULT: ws://localhost:8000
# NOTE: Optional - if not set, inferred from NEXT_PUBLIC_API_URL
NEXT_PUBLIC_WS_URL=ws://localhost:8000

# NEXT_PUBLIC_ENABLE_REASONING_UI: Show reasoning effort indicators in UI
# DEFAULT: true
# PRODUCTION: Set to false to hide internal reasoning details from users
NEXT_PUBLIC_ENABLE_REASONING_UI=true

# ==============================================================================
# MCP SERVER CONFIGURATION
# ==============================================================================
#
# Model Context Protocol (MCP) server configuration
# MCP servers provide external capabilities (web search, UI testing, etc.)
#
# PLAYWRIGHT MCP: UI testing automation
# PLAYWRIGHT_HEADLESS: Run browser in headless mode (no visible window)
#   true: Headless (CI/CD, production)
#   false: Headed (development, debugging)
#
# DEFAULT: true
PLAYWRIGHT_HEADLESS=true

# PLAYWRIGHT_BROWSERS_PATH: Path to Playwright browser installations
# DEFAULT: ~/.cache/ms-playwright
PLAYWRIGHT_BROWSERS_PATH=~/.cache/ms-playwright

# MCP_PERPLEXITY_TIMEOUT: Timeout for Perplexity MCP requests (seconds)
# DEFAULT: 30
MCP_PERPLEXITY_TIMEOUT=30

# MCP_PLAYWRIGHT_TIMEOUT: Timeout for Playwright MCP requests (seconds)
# DEFAULT: 60
MCP_PLAYWRIGHT_TIMEOUT=60

# ==============================================================================
# SECURITY CONFIGURATION
# ==============================================================================
#
# Cryptographic keys and security settings
#
# SECRET_KEY: Flask/FastAPI secret key for session management
#   Generate a secure key: openssl rand -hex 32
#
# SECURITY: MUST be unique and kept secret in production
SECRET_KEY=your_secret_key_here

# JWT_SECRET: JSON Web Token signing secret
#   Generate a secure key: openssl rand -hex 32
#
# SECURITY: MUST be unique and kept secret in production
JWT_SECRET=your_jwt_secret_here

# JWT_ALGORITHM: Algorithm for JWT signing
# DEFAULT: HS256 (HMAC with SHA-256)
JWT_ALGORITHM=HS256

# ACCESS_TOKEN_EXPIRE_MINUTES: JWT token expiration time (minutes)
# DEFAULT: 30 minutes
# RECOMMENDED: 30 for production, 1440 (24h) for development
ACCESS_TOKEN_EXPIRE_MINUTES=30

# BCRYPT_ROUNDS: Number of bcrypt hashing rounds
# DEFAULT: 12
# RANGE: 10-14 (higher = slower but more secure)
BCRYPT_ROUNDS=12

# ==============================================================================
# RATE LIMITING
# ==============================================================================
#
# Protect API from abuse with rate limiting
#
# RATE_LIMIT_REQUESTS: Max requests per period
# DEFAULT: 100 requests
RATE_LIMIT_REQUESTS=100

# RATE_LIMIT_PERIOD: Rate limit time window (seconds)
# DEFAULT: 3600 (1 hour)
RATE_LIMIT_PERIOD=3600

# RATE_LIMIT_REASONING_HIGH: Special limit for high reasoning effort queries
# DEFAULT: 10 requests (more restrictive due to higher cost)
RATE_LIMIT_REASONING_HIGH=10

# RATE_LIMIT_REASONING_PERIOD: Time window for reasoning rate limit (seconds)
# DEFAULT: 3600 (1 hour)
RATE_LIMIT_REASONING_PERIOD=3600

# ==============================================================================
# THEAUDITOR SECURITY SCANNING
# ==============================================================================
#
# TheAuditor provides automated security scanning for AI-generated code
# https://github.com/TheAuditorTool/Auditor
#
# AUDITOR_ENABLED: Enable TheAuditor integration
# DEFAULT: true
AUDITOR_ENABLED=true

# AUDITOR_AUTO_SCAN: Automatically scan code on changes
# DEFAULT: false (manual scans only)
AUDITOR_AUTO_SCAN=false

# AUDITOR_SCAN_ON_DEPLOY: Scan before deployment
# DEFAULT: true (recommended for CI/CD)
AUDITOR_SCAN_ON_DEPLOY=true

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================
#
# Logging verbosity and format
#
# LOG_LEVEL: Minimum log level to output
#   - DEBUG: Verbose debugging info (development)
#   - INFO: General informational messages (production default)
#   - WARNING: Warning messages only
#   - ERROR: Error messages only
#   - CRITICAL: Critical errors only
#
# DEFAULT: INFO
# RECOMMENDED: DEBUG for local, INFO for prod
LOG_LEVEL=INFO

# LOG_FORMAT: Log output format
#   - json: Structured JSON logs (production, log aggregation)
#   - standard: Human-readable text logs (development)
#
# DEFAULT: json
LOG_FORMAT=json

# ENABLE_STRUCTURED_LOGGING: Add structured metadata to logs
# DEFAULT: true
ENABLE_STRUCTURED_LOGGING=true

# LOG_REASONING_DECISIONS: Log reasoning effort decisions
# DEFAULT: true
# BENEFIT: Helps debug and optimize reasoning system
LOG_REASONING_DECISIONS=true

# ==============================================================================
# PERFORMANCE CONFIGURATION
# ==============================================================================
#
# Performance tuning for high-load scenarios
#
# MAX_CONCURRENT_REQUESTS: Maximum simultaneous requests
# DEFAULT: 50
# RECOMMENDED: 50 for local, scale up for production
MAX_CONCURRENT_REQUESTS=50

# REQUEST_TIMEOUT: Global request timeout (seconds)
# DEFAULT: 300 (5 minutes)
REQUEST_TIMEOUT=300

# REASONING_QUEUE_SIZE: Max queued reasoning requests
# DEFAULT: 10
REASONING_QUEUE_SIZE=10

# ENABLE_ASYNC_PROCESSING: Enable asynchronous request processing
# DEFAULT: true
ENABLE_ASYNC_PROCESSING=true

# ==============================================================================
# FEATURE FLAGS
# ==============================================================================
#
# Enable/disable features without code changes
#
# ENABLE_HITL: Human-in-the-loop approval workflow
# DEFAULT: true (Phase 0)
ENABLE_HITL=true

# ENABLE_SUB_AGENTS: Allow agents to spawn sub-agents
# DEFAULT: true (Phase 0)
ENABLE_SUB_AGENTS=true

# ENABLE_RESEARCH_MODE: Deep research with Open Deep Research framework
# DEFAULT: false (Phase 2 feature)
ENABLE_RESEARCH_MODE=false

# ENABLE_COST_TRACKING: Track and log API costs
# DEFAULT: true
ENABLE_COST_TRACKING=true

# ENABLE_MEMORY_SYSTEM: Semantic memory with pgvector
# DEFAULT: false (Phase 1 feature)
ENABLE_MEMORY_SYSTEM=false

# ==============================================================================
# DEVELOPMENT TOOLS
# ==============================================================================
#
# Development and debugging utilities
#
# ENABLE_DEBUG_TOOLBAR: Show FastAPI debug toolbar
# DEFAULT: false
# RECOMMENDED: true for local, false for all other environments
ENABLE_DEBUG_TOOLBAR=false

# ENABLE_PROFILING: Enable performance profiling
# DEFAULT: false
# NOTE: Adds overhead - use only when diagnosing performance issues
ENABLE_PROFILING=false

# MOCK_EXTERNAL_APIS: Mock external API calls for testing
# DEFAULT: false
# RECOMMENDED: true for tests, false for local/dev/staging/prod
MOCK_EXTERNAL_APIS=false

# ==============================================================================
# REPLIT SPECIFIC (WHEN DEPLOYED)
# ==============================================================================
#
# Configuration for Replit deployment
# Only needed if deploying to Replit platform
#
REPL_SLUG=deep-agent-one
REPL_OWNER=your_username
